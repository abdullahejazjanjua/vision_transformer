# Vision Transformer (ViT) Implementation

A PyTorch implementation of Vision Transformer (ViT) from scratch. This repository contains a clean, educational implementation of the attention mechanism and transformer architecture for computer vision tasks.

## Features

- Custom multi-head attention implementation
- Efficient attention computation with 3x speedup
- Educational code with clear variable names and comments
- Modular design for easy understanding

## TODO List

1. [x] `Attention` class
2. [x] `MultiHeadedAttention` class  
3. [x] `EfficientMultiHeadedAttention` class
4. [x] `LayerNormalization` class
5. [ ] `PatchEmbedding` class
6. [ ] `PositionalEncoding` class
7. [ ] `MLP` class
8. [ ] `TransformerBlock` class
9. [ ] `ClassificationHead` class
10. [ ] `ViT` class
11. [ ] CLS token handling
12. [ ] Residual connections
13. [ ] Dropout layers
14. [ ] Weight initialization
